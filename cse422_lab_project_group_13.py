# -*- coding: utf-8 -*-
"""CSE422 Lab Project GROUP 13

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vWB1QEIUDBVVcuwRW6BQK_gzkqtICR76
"""

#importing necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix
from sklearn.cluster import KMeans

"""<h1>2. Dataset Description</h1>"""

loanData = pd.read_csv("https://raw.githubusercontent.com/ahmadeshtiak/Loan-Prediction-Model/refs/heads/main/Loan%20Approval%20Dataset.csv")
loanData

"""<h3>Feature Names and its Datatypes</3>"""

loanData.info()

"""<h3>Numerical Features</h3>"""

##Selecting numerical features
numerical_data = loanData.select_dtypes(include='number')

#append the features of numerical_data to list
numerical_features=numerical_data.columns.tolist()

print(f'There are {len(numerical_features)} numerical features:', '\n')
print(numerical_features)

numerical_data.describe().T

numerical_data.var()

"""Skew"""

numerical_data.skew()

"""<h3>Categorical Features</h3>"""

#Selecting categoricalfeatures
categorical_data=loanData.select_dtypes(include= 'object')

#append the features of categorical_data to list
categorical_features=categorical_data.columns.tolist()

print(f'There are {len(categorical_features)} categorical features:', '\n')
print(categorical_features)

categorical_data.describe().T

# unique values counts
unique_counts=categorical_data.nunique()
print(unique_counts)

for col in categorical_features:
    plt.title(f'Distribution of {col}')
    categorical_data[col].value_counts().sort_index().plot(kind='bar', rot=0, xlabel=col,ylabel='count')
    plt.show()

"""<h3>Histogram And Box Plot</h3>"""

numerical_data.hist(figsize=(12,12),bins=20)
plt.show()

# Select only numerical columns for boxplot analysis
numeric_cols = loanData.select_dtypes(include=['int64', 'float64']).columns

# Set up the figure
plt.figure(figsize=(20, 30))

# Plot boxplots for each numerical feature including the target variable 'OUTCOME'
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(len(numeric_cols), 1, i)
    sns.boxplot(x=loanData[col], color='skyblue')
    plt.title(f'Boxplot of {col}', fontsize=12)
    plt.tight_layout()

plt.show()

"""<h3>Correlation of the Features & Heatmap</h3>"""

# Encoding to get numerical values from categorical values
# Datatype object diye String type data bujhay, Cagegorical column arki
# astype('category') mane string ke category te convert kora
for col in loanData.select_dtypes(include=['object']).columns:
    loanData[col] = loanData[col].astype('category').cat.codes

corr_matrix = loanData.corr()

# Heatmap Plot
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap with Encoded Categorical Features")
plt.show()

fig, ax = plt.subplots(3,1, figsize=(10, 10))
## Correlation coefficient using different methods
corr1 = loanData.corr('pearson')[['loan_status']].sort_values(by='loan_status', ascending=False)
corr2 = loanData.corr('spearman')[['loan_status']].sort_values(by='loan_status', ascending=False)
corr3 = loanData.corr('kendall')[['loan_status']].sort_values(by='loan_status', ascending=False)

#setting titles for each plot
ax[0].set_title('Pearson method')
ax[1].set_title('spearman method')
ax[2].set_title('Kendall method')

## Generating heatmaps of each methods
sns.heatmap(corr1, ax=ax[0], annot=True)
sns.heatmap(corr2, ax=ax[1], annot=True)
sns.heatmap(corr3, ax=ax[2], annot=True)

plt.show()

numerical_data.plot(kind='density',figsize=(14,14),subplots=True,layout=(6,2),title="Density plot of Numerical features",sharex=False)
plt.show()

"""<h3>Checking for Imbalance</h3>"""

# Count the number of instances for each class in the output column
class_counts = loanData[loanData.columns[13]].value_counts()

# Print the counts
print(class_counts)

# Plot bar chart
plt.figure(figsize=(8, 5))
class_counts.plot(kind='bar')
plt.xlabel('Class')
plt.ylabel('Number of Instances')
plt.title(f'Number of Instances per Class in Output Feature ({loanData.columns[13]})')
plt.xticks(rotation=0)
plt.show()

"""# 3. Dataset pre-processing"""

loanData = pd.read_csv("https://raw.githubusercontent.com/ahmadeshtiak/Loan-Prediction-Model/refs/heads/main/Loan%20Approval%20Dataset.csv")
print(loanData.shape)
loanData.head()

loanData.isnull().sum()

"""Handling Null/Missing values


"""

#Dropping rows
print("Shape of dataframe before dropping:", loanData.shape)
loanData = loanData.dropna(axis = 0, subset = ['credit_score'])

#Impute
impute = SimpleImputer(missing_values=np.nan, strategy='mean')
impute.fit(loanData[['person_income']])
loanData['person_income'] = impute.transform(loanData[['person_income']])
print("Shape after dropping:", loanData.shape)

loanData.isnull().sum()

"""Encoding Categorical Values





"""

#unique Categories for each categorical colums
print(loanData['person_gender'].unique())
print(loanData['person_education'].unique())
print(loanData['person_home_ownership'].unique())
print(loanData['loan_intent'].unique())
print(loanData['previous_loan_defaults_on_file'].unique())

# Applying the encoding to the categorical columns and replacing with the originals
enc = LabelEncoder()

loanData['person_gender'] = enc.fit_transform(loanData['person_gender'])
loanData['person_education'] = enc.fit_transform(loanData['person_education'])
loanData['person_home_ownership'] = enc.fit_transform(loanData['person_home_ownership'])
loanData['loan_intent'] = enc.fit_transform(loanData['loan_intent'])
loanData['previous_loan_defaults_on_file'] = enc.fit_transform(loanData['previous_loan_defaults_on_file'])

loanData.head()

"""Feature Scalling"""

X = loanData.drop("loan_status", axis=1)
y = loanData["loan_status"]

# Identifying numerical columns for scaling
numerical_cols_after_encoding = X.select_dtypes(include=np.number).columns.tolist()
scaler = StandardScaler()
X[numerical_cols_after_encoding] = scaler.fit_transform(X[numerical_cols_after_encoding])

print("After Scaling of the features...")
X.head()

"""# 4. Dataset Splitting
Using stratified method for dividing the datasets as the dataset is imbalanced
"""

# First split: Train + Validation vs Test
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)  # 80/20 split

# Second split: Train vs Validation
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=42
)  # 20% of 80% = 16% for validation
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

"""# 5. Model Training and Testing
Supervised Learning Phase
"""

from sklearn.model_selection import GridSearchCV

model_results = {}

# Define models with their parameter grids
param_grids = {
    "K-Nearest Neighbors": {
        "model": KNeighborsClassifier(),
        "params": {"n_neighbors": [3, 5, 7, 9, 11]}
    },
    "Logistic Regression": {
        "model": LogisticRegression(random_state=42, max_iter=1000),
        "params": {"C": [0.01, 0.1, 1, 10], "penalty": ["l2"], "solver": ["lbfgs"]}
    },
    "Neural Network (MLP)": {
        "model": MLPClassifier(random_state=42, max_iter=500),
        "params": {
            "hidden_layer_sizes": [(50,), (100,), (50,50)],
            "alpha": [0.0001, 0.001, 0.01],
            "learning_rate_init": [0.001, 0.01]
        }
    }
}

# Loop through models and tune using GridSearchCV
for name, config in param_grids.items():
    print(f"\nTuning hyperparameters for {name}...")

    grid = GridSearchCV(
        config["model"],
        config["params"],
        cv=5,                # 5-fold cross-validation on training data
        scoring="accuracy",  # metric for model selection
        n_jobs=-1
    )
    grid.fit(X_train, y_train)

    # Best model from GridSearchCV
    best_model = grid.best_estimator_

    # Validation score (from cross-validation)
    val_accuracy = grid.best_score_
    print(f"Best Params: {grid.best_params_}")
    print(f"Validation Accuracy (CV): {val_accuracy:.4f}")

    # Test set evaluation
    y_pred = best_model.predict(X_test)
    y_prob = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, "predict_proba") else None

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    if y_prob is not None:
        fpr, tpr, thresholds = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
    else:
        roc_auc = None

    # Save results
    model_results[name] = {
        "Best Params": grid.best_params_,
        "Validation Accuracy": val_accuracy,
        "Test Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1": f1,
        "ConfusionMatrix": cm,
        "AUC": roc_auc,
        "ROC": (fpr, tpr, thresholds) if roc_auc is not None else None
    }

print("Hyperparameter tuning and evaluation completed successfully!")

"""Unsupervised Learning Phase"""

#Finding the best number of clusters using elbow method
k_range = range(1,13)
SSE = []
for k in k_range:
  km = KMeans(n_clusters= k)
  km.fit(loanData[['loan_amnt','person_income']])
  SSE.append(km.inertia_)
plt.xlabel("K")
plt.ylabel("Sum of Squarred Error [SSE]")
plt.plot(k_range,SSE)

# Applying KMeans with 4 clusters as the elbow point is 4 here
kmeans_df = X[["loan_amnt", "person_income"]]
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(kmeans_df)
# Visualizing the clusters
plt.figure(figsize=(10, 8))
sns.scatterplot(
    x=kmeans_df["person_income"],
    y=kmeans_df["loan_amnt"],
    hue=kmeans_labels,
    palette="viridis",
    s=50,
)
plt.title("K-Means Clustering of Loan Amount and Income")
plt.xlabel("Person Income (Scaled)")
plt.ylabel("Loan Amount (Scaled)")
plt.show()
print("The K-Means algorithm has successfully clustered the data into two distinct groups, which likely correspond to the approved and not-approved loan statuses.")

"""# 6. Model selection/Comparison analysis
Bar chart showcasing prediction accuracy
"""

print("\n--- Model Selection/Comparison Analysis ---")
results_df = pd.DataFrame(model_results).T
print(results_df)

plt.figure(figsize=(12, 6))
sns.barplot(x=results_df.index, y="Test Accuracy", data=results_df)
plt.title("Prediction Accuracy Comparison of All Models")
plt.ylabel("Accuracy")
plt.ylim(0, 1)
plt.xticks(rotation=45, ha="right")
plt.show()

"""Bar chart for precision and recall comparison"""

results_df[["Precision", "Recall"]].plot(
    kind="bar", figsize=(12, 6), rot=45, ylim=(0, 1)
)
plt.title("Precision and Recall Comparison of All Models")
plt.ylabel("Score")
plt.show()

"""Confusion Matrices"""

for model in model_results:
  con = model_results[model]["ConfusionMatrix"]
  plt.figure(figsize=(6, 5))
  sns.heatmap(con, annot=True, fmt="d", cmap="Blues")
  plt.title(f"Confusion Matrix for {model}")
  plt.xlabel("Predicted Label")
  plt.ylabel("True Label")
  plt.show()

"""AUC score and ROC curve"""

for model in model_results:
  roc = model_results[model]["ROC"]
  roc_auc = model_results[model]["AUC"]
  plt.figure(figsize=(6, 5))
  plt.plot(roc[0], roc[1], color="darkorange", lw=2, label=f"ROC curve (area = {roc_auc:.2f})")
  plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel("False Positive Rate")
  plt.ylabel("True Positive Rate")
  plt.title(f"ROC Curve for {model}")
  plt.legend(loc="lower right")
  plt.show()
  print(f"AUC Score: {roc_auc:.4f}")

print("Overall neural network is ahead of performance than other models here")